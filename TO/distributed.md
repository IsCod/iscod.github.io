# 分布式

分布式是指将计算或处理任务分布到多个计算机或服务器上进行并行处理的一种系统架构。

## 分布式设计的目标

1. 可扩展：通过对服务、存储的扩展，提高系统的处理能力
1. 高可用: 单点故障影响整体
1. 无状态：无状态的服务才能满足宕机后不影响全局
1. 可管理：便于运维和监控及时发现问题
1. 高可靠：同样的请求返回的数据一致，更新数据持久化，不丢失

分布式系统虽然提供和很多优点，但也带来了系统的复杂性与分布式特有的一些问题，如分布式ID、分布式事务、分布式锁、分布式session、数据一致性 等都是现代分布式系统中需要解决的难题。
分布式系统同时也增加了开发测试运维成本，工作量增加，分布式系统管理不好反而会变成一种负担。

本文尝试就分布式系统几个问题进行分析和总结，并提出常用的解决方案以供参考学习。

### CAP理论

* C: 一致性（Consistency）

* A: 可靠性（Availability）

* P: 分区容错性（Partition tolerance）

分布式系统有很多节点，节点之间是通过网络进行通信。而网络通信总是不可靠的，一旦网络通信出现问题，此时就称当前的分布式系统出现了分区。

理解CAP理论，要首先明白CAP理论并不是工程理论，而是一个学术理论。CAP理论也不仅仅只用在软件工程。CAP理论的基础是`描述状态`。

CAP理论表达了一个分布式系统里不可能同时满足三个特性（C,A,P）

而分布式系统，P是必然会发生，必须进行选择一项，而C和A就只能优先满足一种。

## 分布式ID

### 场景

在分布式系统中，多个web服务客户端需要连接DB进行获取唯一ID，在单体应用中，可以使用数据库 `AUTO_INCREMENT`，或者redis的`INCR`。
随着业务量的增加，单个数据库服务器无法满足性能要求，因此有提出对数据库的分片和水平扩展，设置发号器和步距。然而该方法在后期扩展时，需要考虑步距与数据库分片数量关系，新的起始ID与现有业务ID冲突问题等。所以并非最好的解决方案而只能作为一种中间方案过渡。

#### 解决方案

* 数据库发号器与步距

数据库可设置两个分表，分表设置的`AUTO_INCREMENT` 根据 `step` 步长进行设置

* [雪花算法](https://iscod.github.io/#/sort/snowflake)

雪花算法ID结构：

```
+--------------------------------------------------------------------------+
| 1 Bit Unused | 41 Bit Timestamp |  10 Bit NodeID  |   12 Bit Sequence ID |
+--------------------------------------------------------------------------+
```

该结构保证同一时间，同一机器下每毫秒能产生4096个唯一ID，而这样的结构基本满足日常需求。
但作为高并发的分布式应用，需要解决雪花算法的两个问题：`序列超过4096`和`时钟回拨`

    * 序列超过4096

        一般采用程序阻塞, 阻塞到下一个时间毫秒时生产新的ID。

    * 时钟回拨

        一般分为三种情况，极小时间回拨，稍长时间回拨，和较长时间回拨。而对于三种情况需要不同的解决方案：[雪花算法](https://iscod.github.io/#/sort/snowflake)

## 分布式事务

分布式事务是指在分布式系统中，涉及到多个独立的服务或数据库操作，需要保证这些操作要么全部成功，要么全部失败的一种事务处理方式。

### 场景

随着业务的快速发展，几乎每个公司的系统都会从单体走向分布式，特别是转向微服务架构的系统。

银行跨行转账业务是一个典型分布式事务场景：假设A需要跨行转账给B，那么就涉及两个银行的数据，无法通过一个数据库的本地事务保证转账的ACID，只能够通过分布式事务来解决。

### 解决方案

1. 基于XA协议的两阶段和三阶段提交，需要数据库层面支持
1. 基于事务补偿的TCC(Try, Confirm, Cancel), 基于业务层面实现
1. 基于事务消息mq, 需要消息队列支持

分布式事务的解决方案目前比较成熟两种项目：[DTM](https://github.com/dtm-labs/dtm)、[seata](https://github.com/seata/seata)

#### 二阶段提交

1. 第一阶段（prepare）: TM（事务管理器）向每个参与者发送 preCommit消息, 参与者执行本地事务，但不提交，进入 ready 状态，并通知TM（事务管理器）已经准备就绪
1. 第二阶段（commit）: TM（事务管理器）向参与者发送 commit 消息，参与者执行本地事务提交，TM（事务管理器）确认每个参与者都 ready 后，通知参与者进行 commit 提交，如果有参与者 fail, 则发送 rollback 命令，各参与者做回滚

问题：
    * 单点故障: 一但TM（事务管理器）出现故障，整个系统便不可用（参与者都会阻塞）
    * 数据不一致：在二阶段如果TM（事务管理器）只发送了部分commit消息，此时网络发生异常，那么只有部分参与者收到commit消息，提交了事务，使系统数据不一致
    * 响应时间较长：参与者与协调者资源被长时间锁住，提交与回滚后才能解除
    * 不确定性：当协调事务管理器发送commit后，只有部分参与者收到commit消息，此时发生宕机，重新选举的事务管理器，无法确定该消息是否成功。

#### 三阶段提交

三阶段提交比二阶段做了两个优化

* 增加一个`canCommit`阶段，确认参与者环境
* 引入参与者超时机制（参与者在第三阶段长时间未收到commit消息则自身执行commit操作），降低单点故障问题

##### 三阶段流程

1. 第一阶段（canCommit）: TM（事务管理器）向每个参与者发送 canCommit 消息, 确认参与者环境正常
1. 第二阶段（prepare）: TM（事务管理器）向每个参与者发送 preCommit消息, 参与者执行本地事务，但不提交，进入 ready 状态，并通知TM（事务管理器）已经准备就绪
1. 第三阶段（commit）: TM（事务管理器）向参与者发送 commit 消息，参与者执行本地事务提交，TM（事务管理器）确认每个参与者都 ready 后，通知参与者进行 commit 提交，如果有参与者 fail, 则发送 rollback 命令，各参与者做回滚

#### TCC事务模型

* TCC分为3个阶段

Try 尝试: 尝试执行，完成所有的业务检查，预留必须业务资源
Confirm 确认: 如果所有的 Try 都成功，则进行 Confirm 。Confirm 真正的去执行业务，该阶段不再做业务检查，只使用 Try 阶段的预留资源
Cancel 取消：如果所有分支的中有一个 Try 失败，则进行 Cancel。Cancel 释放 Try阶段预留的业务资源

* TCC空回滚

在调用Try阶段，由于网络延迟，或者故障等原因，没有执行，结果返回异常。
而此时Cancel就不能正常执行，因为Try阶段没有对数据进行修改。如果Cancel进行对数据进行了修改，则会导致数据的不一致

解决的思路的关键是要识别出是要知道Try阶段是否执行，如果执行成功就正常回滚，如果失败，则进行空回滚。
一种解决方案是在TM发起全局事务时生成全局事务记录，全局事务ID贯穿整个分布式事务调用链，同时增加一个`分布式事务记录表`，其中记录全局事务ID和分布式事务ID，Try进行插入数据表示执行成功。
Cancel接口读取该记录，如果该记录存在则进行正常回滚，否则进行空回滚

* TCC悬挂问题

Try阶段一般设有超时时间，当某一个分布式服务发生故障，网络延迟一直没处理Try, 当到达超时时间时，全局事务会执行Cancel。
当Cancel执行完毕后，分布式事务的Try才开始运行（比如网络才到达），那么Try的资源就没有触发释放时机。解决该问题的思路是在Try阶段执行前检查是否执行了第二阶段（Cancel/Confirm）,如果已经执行过第二阶段，则不在执行Try。
可通过添加分布式事务表进行状态标记。


### DTM

DTM的典型应用案例有四个：

1. 订单系统

    绝大多数的订单系统都已经服务化，会将订单系统拆分为订单服务、库存服务、优惠券服务、支付服务、账户服务等等。
    通常一个下单操作会涉及：创建订单、扣减库存、扣减优惠券、创建支付单等等，。

    订单场景流程：

    * 创建订单：需要在订单表中创建订单，唯一键为订单ID
    * 扣减库存：需要给用户下单的商品扣减库存
    * 扣减优惠券：用户在下单前，选择了可使用的优惠券，提交订单时，则扣减这部分优惠券
    * 创建支付单：提交订单后，需要创建支付单，最后告诉用户跳转到支付页

    上述场景，如果在单体订单系统中，很容易使用数据库事务来解决。但是在分布式系统中，如果这些系统有一个中间发生进程crash，那么会导致几个服务间数据不一致的问题。
    `dtm`提出采用`saga`事务的方式进行下单流程。

    当发生进程cras时，dtm会进行重试，保证操作会最终完成。如果发生某一进程失败，比如扣减库存时库存不足，则会进行回滚，来保证最终的一致性。

    ```go
        app.POST("/api/busi/submitOrder", common.WrapHandler(func(c *gin.Context) interface{} {
        req := common.MustGetReq(c)
        saga := dtmcli.NewSaga(conf.DtmServer, "gid-"+req.OrderID).
            Add(conf.BusiUrl+"/orderCreate", conf.BusiUrl+"/orderCreateRevert", &req).
            Add(conf.BusiUrl+"/stockDeduct", conf.BusiUrl+"/stockDeductRevert", &req).
            Add(conf.BusiUrl+"/couponUse", conf.BusiUrl+"couponUseRevert", &req).
            Add(conf.BusiUrl+"/payCreate", conf.BusiUrl+"/payCreateRevert", &req)
            return saga.Submit()
        }))
    ```

    在这个代码中，定义了一个saga事务，包含上述下单过程中需要的四个步骤，以及四个步骤需要的补偿操作。

1. 秒杀

    现有的秒杀架构，为了支持高并发，通常把库存放在Redis中，收到订单请求时，在Redis中进行库存扣减。这种的设计，导致创建订单和库存扣减不是原子操作，如果两个操作中间，遇到进程crash等问题，就会导致数据不一致。
    `dtm` 提出了一种全新的秒杀架构，完全解决了秒杀架构中的核心痛点。该架构能够支持每秒超万单精准扣库存，并且保证创建订单和扣减库存的数据最终严格一致。详情参见 秒杀系统

1. 缓存一致性

    缓存是现在系统中必备的基础设施，用来缓解数据库的压力。
    但是一旦引入了缓存，那么数据就在两个地方进行了存储，那么如何才能够保证两者的一致性呢？
    业界已经有很多方案，典型的会依赖消息队列或者订阅数据库的binlog，这些方案都比较重，需要维护消息队列或canel，是个不小的负担。

`DTM`有以下几个特点：

* 简单易用的接口
* 支持多语言栈，这一点非常适合很多其它采用不同语言的小公司，对大公司多语言栈也非常方便

## 分布式锁

### 场景

传统单机部署情况下，可以使用程序自身的相关锁控制（如Java的如ReentrantLock或synchronized， golang的sync.Mutex）。
这种原生的锁机制可以保证在同一个虚拟机进程内同步执行，避免无序现象。

但在互联网场景中，例如商品秒杀，随着系统的并发飙升，需要多台机器并发执行时就会失效。
假设此时两个用户请求同时到来，但是落到了不同的机器上进行业务处理。虽然这两个请求是同时执行的，但是因为在两个不同的机器内，其程序原生锁只对属于自己的虚拟机进程有效。
因此，这种程序的原生锁机制在多机部署场景下就失效了。这是因为两台机器加的锁不是同一个锁(两个实例的不同锁)，这样便会出现库存超卖的现象。

### 解决方案

基于存在的现状，只要保证两台机器加的锁是同一个锁，用加锁的方式对某种资源进行顺序访问控制就可以解决了。而这恰恰就是分布式锁的应用。

分布式锁的思路是：

在整个系统提供一个全局、唯一的获取锁的“东西”，然后每个系统在需要加锁时，都去问这个“东西”拿到一把锁，这样不同的系统拿到的锁就可以认为是同一把锁。

当前分布式加锁主要有三种方式：（磁盘）数据库、缓存数据库、Zookeeper。

#### (磁盘）数据库实现分布式锁(MySQL)

`MySQL`实现分布式锁主要是使用`Mysql`的唯一主键冲突，进行锁的获取

1. Mysql新建一个锁的表

#### Redis缓存实例实现分布式加锁

Redis类型实现的分布式锁，有几大优势：

1. 加锁操作简单，使用`SET`、`GET`、`DEL`等几条简单的命令即可实现锁的获取和释放
1. 性能优越，缓存数据的读写优于磁盘数据库与Zookeeper
1. 可靠性强，Redis可采用主备和集群实例，避免单点故障
1. 对于分布式应用加锁，能够避免出现库存超卖及无序列访问现象

##### redis锁需要解决的问题

1. redis死锁
1. 锁过期时间过早

当程序还没有执行完，锁已过期，其它程序就会占用新的锁，造成程序加锁失败。
而当程序支持完后，释放锁时，释放了别的锁。

解决锁过期的方案主要：设置一个`watch dog`，在程序未执行完的时间段进行锁的续期，如果检查到锁的钥匙变更（程序退出，被其他程序占用了锁）则不进行续期
释放锁时，检查是否是自己的锁才进行释放

##### redis分段锁进行性能提升

当业务商品较多时，例如几万个，那么将库存进行分段，然后进行根据分段设置不同的锁，用以提高性能

##### redis红锁解决redis集群故障时的超卖问题

```lua
-- https://github.com/iscod/iscod.github.io/tree/master/example/redis
local key = KEYS[1]
local required = KEYS[2]
local ttl = tonumber(KEYS[3])
local result = redis.call('SETNX', key, required)

if result == 1 then
    --设置成功，则设置过期时间
    redis.call('PEXPIRE', key, ttl)
else
    local value = redis.call('get', key)
    if value == result then
    --如果跟之前的锁一样，则重新设置时间
        result = 1
        redis.call('PEXPIRE', key, ttl)
    end
end
--成功则返回1
return result
```
```lua
--当锁匹配的钥匙相同时才可以删除锁
local key = KEYS[1]
local required = KEYS[2]
local value = redis.call('GET', key)
if value == required then
    redis.call('DEL', key);
    return 1;
end
return 0;
```

#### Zookeeper

## 分布式session

### 场景

传统单机部署情况下，`SESSION`通常是保存在程序内存当中。
在互联网场景下，由于客户端随时可以链接到的不同的机器，那么`SESSION`就面临着同步的问题。

### 解决方案

而处理`SESSION`的思路大致分有三种：

1. 客户端进行存储

    在分布式环境中，一个客户端的多个请求可能落到不同的服务器上，那么直接将SESSION存储到客户端中(cookie)，就将保证了SESSION的一致性。
    但这样的安全性就非常低，只能存储一些不重要的数据。

1. `SESSION`复制

    将服务器的`SESSION`复制到其它服务器，这样所有的服务器的SESSION就一致了，像`tomcat`等web容器都支持session复制的功能。
    但是需要解决`SESSION`数据的先后版本问题，同时造成了服务器内存资源的浪费。且当服务器数量较多时也面临着同步的延迟和失败问题

1. `IP`绑定策略

    使用`NGINX`中的IP绑定策略，同一个IP只能指定到同一个机器访问，但是这样做失去了负载均衡的意义，发生宕机时业务影响较大。

1. `SESSION`集中管理

    集中管理是目前分布式系统下的主流解决方案。

    这种方式就是将`SESSION`保存到其它存储介质，而服务器分别去访问和使用该存储介质。例如可以使用`redis`,`etcd`, `mysql`等。
    很多的语言框架也都集成了该方案，例如: [php-Laravel](https://laravel.com/docs/10.x/session#main-content), [go-session](https://github.com/go-session/redis), [spring-session](https://spring.io/projects/spring-session)

    虽然架构上变得复杂一些，需要多次访问存储介质，但是这种方案也带来了很多好处：

    * 实现真正的SESSION的共享
    * 可以水平扩展
    * 服务器重启SESSION不丢失
    * 不仅仅可以跨服务器共享，甚至可以跨平台（例如网页端，APP端）

## 分布式定时任务

## 数据一致性

[^1]: <分布式系统原理与范型>

* 参考

    * [分布式系统CAP理论](https://cloud.tencent.com/developer/article/1860632)
    * [redissync](https://github.com/go-redsync/redsync)
    * [redislock](https://github.com/bsm/redislock)
    * [saga](https://microservices.io/patterns/data/saga.html?spm=a2c6h.12873639.article-detail.7.3fab47f488htjP)
    * [DTM](https://www.dtm.pub/app/intro.html)
    * [cookie](https://iscod.github.io/#/net/http%E5%AE%89%E5%85%A8?id=cookie)
    * [redis实现分布式锁](https://iscod.github.io/#/nosql/redis?id=lua%e5%ae%9e%e7%8e%b0%e5%88%86%e5%b8%83%e5%bc%8f%e9%94%81)
    * [DistributedLock](https://support.huaweicloud.com/bestpractice-dcs/dcs-bp-0713001.html)
